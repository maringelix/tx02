# Configura√ß√£o de Alertas no Slack

## ‚úÖ Status Atual

O Alertmanager est√° **rodando e funcional**, mas os alertas do Slack est√£o **desabilitados** para evitar erros de configura√ß√£o.

```
alertmanager-kube-prometheus-stack-alertmanager-0   2/2   Running
```

## üîî Como Habilitar Alertas no Slack

### 1. Criar Webhook do Slack

1. Acesse: https://api.slack.com/apps
2. **Create New App** ‚Üí From scratch
3. Nome: `DX02 Alerts` | Workspace: Seu workspace
4. **Incoming Webhooks** ‚Üí Activate
5. **Add New Webhook to Workspace**
6. Selecionar canal: `#dx02-alerts`
7. Copiar Webhook URL (ex: `https://hooks.slack.com/services/TXXXXXX/BXXXXXX/XXXXXXXXXXXXXXX`)

### 2. Adicionar Webhook nos GitHub Secrets

```bash
# No reposit√≥rio TX02 no GitHub:
Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí New repository secret

Name: SLACK_WEBHOOK_URL
Value: https://hooks.slack.com/services/TXXXXXX/BXXXXXX/XXXXXXXXXXXXXXX
```

### 3. Atualizar prometheus-values.yaml

Edite `k8s/observability/prometheus-values.yaml`:

```yaml
# Alertmanager Configuration
alertmanager:
  enabled: true
  
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/TXXXXXX/BXXXXXX/XXXXXXXXXXXXXXX'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'slack-notifications'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
      - match:
          severity: critical
        receiver: 'slack-critical'
        continue: true
      - match:
          severity: warning
        receiver: 'slack-notifications'
        continue: true
    
    receivers:
    - name: 'null'
    
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#dx02-alerts'
        title: '{{ .Status | toUpper }} - DX02 Alert'
        text: |
          *Alert:* {{ .CommonLabels.alertname }}
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Cluster:* tx02-prd-aks
          *Namespace:* {{ .CommonLabels.namespace }}
        send_resolved: true
    
    - name: 'slack-critical'
      slack_configs:
      - channel: '#dx02-critical'
        title: ':fire: CRITICAL ALERT - DX02'
        text: |
          <!channel>
          *Alert:* {{ .CommonLabels.alertname }}
          *Severity:* CRITICAL
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Cluster:* tx02-prd-aks
        send_resolved: true
```

### 4. Aplicar Configura√ß√£o

**Via Workflow (Recomendado)**:
```
Actions ‚Üí üìä Deploy Observability Stack ‚Üí Run workflow
Input: Cole seu Slack Webhook URL
```

**Via Helm Local**:
```powershell
cd C:\Files\Learn\Projetos\tx01\tx02

helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack `
  --namespace monitoring `
  --values k8s/observability/prometheus-values.yaml
```

## üß™ Testar Alertas

### 1. For√ßar Alerta de Pod Down

```powershell
# Deletar pod do DX02 para gerar alerta
kubectl delete pod -n dx02 -l app=dx02
```

Ap√≥s 5 minutos, deve receber alerta no Slack:

```
üî¥ FIRING - DX02 Alert

Alert: DX02PodDown
Severity: critical
Summary: DX02 pod is down
Description: DX02 pod has been down for more than 5 minutes
Cluster: tx02-prd-aks
Namespace: dx02
```

### 2. Ver Alertas Ativos

```powershell
# Port-forward Alertmanager
kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093
```

Acesse: **http://localhost:9093**

### 3. Ver Regras no Prometheus

```powershell
# Port-forward Prometheus
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
```

Acesse: **http://localhost:9090/alerts**

## üìä Regras de Alerta Configuradas

### 1. DX02PodDown (Critical)
- **Condi√ß√£o**: Pod n√£o est√° running por > 5min
- **Canal**: #dx02-critical

### 2. DX02HighErrorRate (Critical)
- **Condi√ß√£o**: Taxa de erro > 5% por 5min
- **Canal**: #dx02-critical

### 3. DX02SlowResponse (Warning)
- **Condi√ß√£o**: Tempo de resposta > 1s por 10min
- **Canal**: #dx02-alerts

### 4. DX02HighMemory (Warning)
- **Condi√ß√£o**: Uso de mem√≥ria > 80% por 10min
- **Canal**: #dx02-alerts

### 5. DX02HighCPU (Warning)
- **Condi√ß√£o**: Uso de CPU > 80% por 10min
- **Canal**: #dx02-alerts

### 6. DX02DatabaseConnection (Critical)
- **Condi√ß√£o**: Erro de conex√£o com database
- **Canal**: #dx02-critical

## üîß Troubleshooting

### Alertmanager n√£o est√° enviando alertas

```powershell
# Verificar logs
kubectl logs -n monitoring alertmanager-kube-prometheus-stack-alertmanager-0 -c alertmanager

# Verificar configura√ß√£o
kubectl get secret -n monitoring alertmanager-kube-prometheus-stack-alertmanager -o yaml
```

### Webhook URL inv√°lida

Erro: `unsupported scheme "" for URL`

**Solu√ß√£o**: Certifique-se que o Slack webhook est√° no formato correto:
```
https://hooks.slack.com/services/TXXXXXX/BXXXXXX/XXXXXXXXXXXXXXX
```

### Alertas n√£o est√£o disparando

1. Verificar regras no Prometheus: http://localhost:9090/alerts
2. Verificar ServiceMonitor: `kubectl get servicemonitor -n monitoring`
3. Verificar se m√©tricas est√£o sendo coletadas: http://localhost:9090/targets

## üìù Configura√ß√£o Atual (Sem Slack)

Por padr√£o, o Alertmanager est√° configurado com receiver `null` (sem notifica√ß√µes) para garantir que rode sem erros.

**Para habilitar Slack**: Siga os passos 1-4 acima.

---

**Status**: ‚è≥ Alertmanager funcional, Slack n√£o configurado  
**Pr√≥ximo passo**: Adicionar SLACK_WEBHOOK_URL e atualizar values.yaml
